This chapter will guide you through the mainstream "connectionism" AI, which is fundamental for understanding what the majority of researchers are currently working on, where we are at, what are to improve, and what ideas and philosophy we can borrow to create something similar and hopefully better.

# What is "Connectionism"?
Connectionism suggests that the interconnection between trivial systems (neurons) is the key to intelligence. Deep learning, in particular, belongs to connectionism since it is based on multiple layered artificial neural network.

While "non-deep learning" machine learning algorithms does not typically involve a neural network, as these algorithms involve, they begin to rely more and more on techniques like undetermined parameters and gradient descend, so they begin to have more and more "connectionism" characteristics. For example, the decision tree algorithm was initially a purely logical model that learns by examining the training data with statistical approaches, but it then evolved into random forest, and then adaboost, and finally xgboost, and to this point, while the overall performance gets better, it is flooded with tons of unexplainable weights and parameters. Therefore, while "non-deep learning" machine learning algorithms are mainly based on statistical approaches, they evolve to be more and more like "connectionism" for better performance, we can roughly regard them as "connectionism" too.

Now we can see that almost all artificial intelligence approaches that we learn about today belongs to connectionism, but connectionism is not the only way to implement an AI. "Symbolism", for example, is a paradigm that suggests intelligence is an activity of logical reasoning and can be modeled by manipulating symbols.



$$a=\frac{1}{2}$$